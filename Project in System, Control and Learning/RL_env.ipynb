{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b3d185-2063-416a-85d8-89fa1b7af38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling FurutaPendulums [8f95568d-7bec-4a61-b2a3-e2d98ddd1432]\n"
     ]
    }
   ],
   "source": [
    "using FurutaPendulums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7957f9af-4fda-45d4-87a5-c6781345de9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Package IntervalSets not found in current path.\n- Run `import Pkg; Pkg.add(\"IntervalSets\")` to install the IntervalSets package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package IntervalSets not found in current path.\n- Run `import Pkg; Pkg.add(\"IntervalSets\")` to install the IntervalSets package.",
      "",
      "Stacktrace:",
      " [1] macro expansion",
      "   @ .\\loading.jl:1163 [inlined]",
      " [2] macro expansion",
      "   @ .\\lock.jl:223 [inlined]",
      " [3] require(into::Module, mod::Symbol)",
      "   @ Base .\\loading.jl:1144"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "using IntervalSets\n",
    "using Plots, ReinforcementLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4fe41-7949-4cdf-b683-ff37518cc5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FurutaEnv <: AbstractEnv\n",
    "    state\n",
    "    reward::AbstractFloat\n",
    "    action_space\n",
    "    action::AbstractFloat\n",
    "    state_space\n",
    "    done\n",
    "    furuta\n",
    "    t\n",
    "    dt\n",
    "    tmax\n",
    "end\n",
    "\n",
    "function FurutaEnv(;\n",
    "        max_u=5.,\n",
    "        max_dθ=100.,\n",
    "        max_dϕ=100.,\n",
    "        dt = 0.002,\n",
    "        tmax = 10.\n",
    "        )\n",
    "    high = [2pi,max_dθ,2pi,max_dϕ]\n",
    "    low = [0,-max_dθ,0,-max_dϕ]\n",
    "    furuta = SimulatedFurutaPendulum()\n",
    "    FurutaEnv(\n",
    "        furuta.x,\n",
    "        0.,\n",
    "        ClosedInterval.(-max_u,max_u),\n",
    "        0.,\n",
    "        Space(ClosedInterval{Float64}.(low, high)),\n",
    "        false,\n",
    "        furuta,\n",
    "        0.,\n",
    "        dt,\n",
    "        tmax\n",
    "        )\n",
    "end\n",
    "\n",
    "RLBase.action_space(env::FurutaEnv) = env.action_space\n",
    "RLBase.state_space(env::FurutaEnv) = env.state_space\n",
    "function RLBase.reward(env::FurutaEnv)\n",
    "    ϕ, ϕdot, θ, θdot = env.furuta.x\n",
    "    costs = θ^2 + 0.1 * θdot^2 + 0.001 * env.action^2\n",
    "    return -costs\n",
    "end\n",
    "RLBase.is_terminated(env::FurutaEnv) = env.done\n",
    "RLBase.state(env::FurutaEnv) = env.state\n",
    "\n",
    "function (env::FurutaEnv)(a::AbstractFloat)\n",
    "    @assert a in env.action_space\n",
    "    env.action = a\n",
    "    dt = env.dt\n",
    "    control(env.furuta,a)\n",
    "    periodic_wait(env.furuta,env.t,dt)\n",
    "    env.state[:] = env.furuta.x[:]\n",
    "    env.t += dt\n",
    "    env.done = env.t >= env.tmax\n",
    "    nothing\n",
    "end\n",
    "\n",
    "function RLBase.reset!(env::FurutaEnv)\n",
    "    env.action = 0.\n",
    "    env.reward = 0.\n",
    "    env.t = 0.\n",
    "    env.furuta = SimulatedFurutaPendulum()\n",
    "    env.state = env.furuta.x\n",
    "    env.done = false\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4378c5eb-ebfc-42f6-b118-08b6a4c64e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FurutaEnv(max_u=5,\n",
    "        max_dθ=100,\n",
    "        max_dϕ=100)\n",
    "env(2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958456ec-8898-4dc3-8bf6-543d056fa262",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(\n",
    "           RandomPolicy(),\n",
    "           FurutaEnv(),\n",
    "           StopAfterEpisode(10),\n",
    "           TotalRewardPerEpisode()\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af40e465-2422-4296-b40b-1f8b00a9be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"sim/env.jl\")\n",
    "function policy(x,t)\n",
    "    ifelse(t<2,2.5,0.)\n",
    "end\n",
    "env = FurutaEnv()\n",
    "dt = 0.002\n",
    "tmax = 10.\n",
    "tspan = 0.:dt:tmax\n",
    "xs = zeros(length(tspan),4)\n",
    "for (i,t) in enumerate(tspan)\n",
    "    xs[i,:] = env.state\n",
    "    u = policy(env.state,t)\n",
    "    env(u)\n",
    "end\n",
    "plot(tspan,xs)\n",
    "animate_pendulum(xs[:,1],xs[:,3],tmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d206df-4cfc-48a9-adf9-c818e582dbb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: Package StableRNGs not found in current path.\n- Run `import Pkg; Pkg.add(\"StableRNGs\")` to install the StableRNGs package.",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package StableRNGs not found in current path.\n- Run `import Pkg; Pkg.add(\"StableRNGs\")` to install the StableRNGs package.",
      "",
      "Stacktrace:",
      " [1] macro expansion",
      "   @ .\\loading.jl:1163 [inlined]",
      " [2] macro expansion",
      "   @ .\\lock.jl:223 [inlined]",
      " [3] require(into::Module, mod::Symbol)",
      "   @ Base .\\loading.jl:1144"
     ]
    }
   ],
   "source": [
    "using ReinforcementLearning\n",
    "using StableRNGs\n",
    "using Flux\n",
    "using Flux.Losses\n",
    "using IntervalSets\n",
    "\n",
    "function RL.Experiment(\n",
    "    ::Val{:JuliaRL},\n",
    "    ::Val{:DDPG},\n",
    "    ::Val{:Pendulum},\n",
    "    ::Nothing;\n",
    "    seed = 123,\n",
    ")\n",
    "    rng = StableRNG(seed)\n",
    "    inner_env = FurutaEnv()\n",
    "    A = action_space(inner_env)\n",
    "    low = A.left\n",
    "    high = A.right\n",
    "    ns = length(state(inner_env))\n",
    "\n",
    "    env = ActionTransformedEnv(\n",
    "        inner_env;\n",
    "        action_mapping = x -> low + (x + 1) * 0.5 * (high - low),\n",
    "    )\n",
    "    init = glorot_uniform(rng)\n",
    "\n",
    "    create_actor() = Chain(\n",
    "        Dense(ns, 30, relu; init = init),\n",
    "        Dense(30, 30, relu; init = init),\n",
    "        Dense(30, 1, tanh; init = init),\n",
    "    ) |> gpu\n",
    "\n",
    "    create_critic_model() = Chain(\n",
    "        Dense(ns + 1, 30, relu; init = init),\n",
    "        Dense(30, 30, relu; init = init),\n",
    "        Dense(30, 1; init = init),\n",
    "    ) |> gpu\n",
    "\n",
    "    create_critic() = TD3Critic(create_critic_model(), create_critic_model())\n",
    "\n",
    "    agent = Agent(\n",
    "        policy = DDPGPolicy(\n",
    "            behavior_actor = NeuralNetworkApproximator(\n",
    "                model = create_actor(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            behavior_critic = NeuralNetworkApproximator(\n",
    "                model = create_critic(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            target_actor = NeuralNetworkApproximator(\n",
    "                model = create_actor(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            target_critic = NeuralNetworkApproximator(\n",
    "                model = create_critic(),\n",
    "                optimizer = ADAM(),\n",
    "            ),\n",
    "            γ = 0.99f0,\n",
    "            ρ = 0.99f0,\n",
    "            batch_size = 64,\n",
    "            start_steps = 1000,\n",
    "            start_policy = RandomPolicy(-1.0..1.0; rng = rng),\n",
    "            update_after = 1000,\n",
    "            update_freq = 1,\n",
    "            policy_freq = 2,\n",
    "            target_act_limit = 1.0,\n",
    "            target_act_noise = 0.1,\n",
    "            act_limit = 1.0,\n",
    "            act_noise = 0.1,\n",
    "            rng = rng,\n",
    "        ),\n",
    "        trajectory = CircularArraySARTTrajectory(\n",
    "            capacity = 10_000,\n",
    "            state = Vector{Float32} => (ns,),\n",
    "            action = Float32 => (),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    stop_condition = StopAfterStep(10_000, is_show_progress=!haskey(ENV, \"CI\"))\n",
    "    hook = TotalRewardPerEpisode()\n",
    "    Experiment(agent, env, stop_condition, hook, \"# Play Pendulum with TD3\")\n",
    "end\n",
    "\n",
    "using Plots\n",
    "ex = E`JuliaRL_DDPG_Pendulum`\n",
    "run(ex)\n",
    "plot(ex.hook.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29aafdb-63f4-4140-9520-3215566fa13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "T = Float64\n",
    "p_env = PendulumEnv(;\n",
    "    max_speed = T(8),\n",
    "    max_torque = T(2),\n",
    "    g = T(10),\n",
    "    m = T(1),\n",
    "    l = T(1),\n",
    "    dt = T(0.05),\n",
    "    max_steps = 200,\n",
    "    continuous = true,\n",
    "    n_actions = 3,\n",
    "    rng = Random.GLOBAL_RNG,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
