{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c997d48-5ffd-47e8-ace6-51fd56570f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base: +, *\n",
    "using Flux\n",
    "import Flux: params\n",
    "using Flux.Optimise\n",
    "using Flux: crossentropy, ADAM\n",
    "using LinearAlgebra\n",
    "using Plots\n",
    "import ProgressMeter\n",
    "import ProgressMeter: @showprogress\n",
    "PM = ProgressMeter\n",
    "PM.ijulia_behavior(:clear)\n",
    "using Statistics\n",
    "import Random: seed!\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49bc20-5df1-468c-aeb0-16f6920383da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the reward function. You can adjust it however you like.\n",
    "stage_reward(x; sp, ap) = Δt*( - x[3]*ap - x[4]*ifelse(x[3] < π/2, 1, -sp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47358b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the total reward\n",
    "function Total_reward(x_init, n_steps, cp, sp)\n",
    "    x = x_init\n",
    "    c = 0\n",
    "    for k in 1:n_steps\n",
    "        u = policy(x)\n",
    "        x = sim(x,u)\n",
    "        c += stage_reward(x, cp=cp, sp=sp)\n",
    "    end\n",
    "    return c\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82093651-bf8f-4a73-9fa1-c34f8e6eeb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "function play(p :: Net; x_init, n_steps, cp, sp, ϵ)\n",
    "    log = []\n",
    "    x = x_init\n",
    "    for k in 1:n_steps\n",
    "        if rand() > ϵ\n",
    "            u = p(x)\n",
    "        else\n",
    "            u = [5*rand() - 1.0]\n",
    "        end\n",
    "        x₊ = sim(x,u)\n",
    "        sc = stage_reward(x₊, cp=cp, sp=sp)\n",
    "        push!(log, (x=x, u=u, x₊=x₊, sc=sc))\n",
    "        x = x₊\n",
    "    end\n",
    "    log\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10332a98-51dc-4794-9713-1df27002ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "function run(p :: Net; x_init, n_steps)\n",
    "    x = x_init\n",
    "    xs = []\n",
    "    for k in 1:n_steps\n",
    "        u = p(x)\n",
    "        x = sim(x,u)\n",
    "        push!(xs, x)\n",
    "    end\n",
    "    xs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1879d492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log[1:end, 1:2] = [200 201; 200 201; 200 201; 200 201]\n",
      "log[1:end, 4] = [4, 4, 4, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function policy_gradient(u)\n",
    "    # The current policy is a simple linear policy.\n",
    "    u = rand()\n",
    "    grad = gradient(u -> Q_function(x,u), u)\n",
    "    return u\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fda455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function estimates the Q value of a state-action pair using a neural network\n",
    "function Q_function(x,u)\n",
    "    # x,u -> Neural network -> estimated Q value\n",
    "    q_train = log[1:end,1:2]\n",
    "    q_target = log[1:end,4]\n",
    "end\n",
    "\n",
    "# This is the Q-learning algorithm\n",
    "function Q_learning(x,u,steps)\n",
    "    for i in 1:steps\n",
    "        x₊, reward = sim(x,u)\n",
    "        x, u = round(x), round(u)\n",
    "        q_values[x,u] += γ * (reward + α*np.max(q_values[round(x₊)]) - q_values[x, u])\n",
    "        push!(log, [x u x₊ q_values[x, u]])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43690863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the functions above to implement your own RL algorithm\n",
    "# As an example, we implement the DDPG algorithm\n",
    "function RL_MODEL(x_init, n_steps, critic_cost_bound, n_iters, replay_buffer_size, n_samples, p_layers, q_layers, γ, ap, sp)\n",
    "    # Pre-training\n",
    "    Q_learning(x_init, u_init, 500)\n",
    "    # Initialize the replay buffer\n",
    "    replay_buffer = []\n",
    "    # Initialize the critic and actor networks\n",
    "    p = Net(p_layers)\n",
    "    q = Net(q_layers)\n",
    "    # Initialize the target networks\n",
    "    p̂ = deepcopy(p)\n",
    "    q̂ = deepcopy(q)\n",
    "    # Initialize the optimizer\n",
    "    opt_p = ADAM(0.001)\n",
    "    opt_q = ADAM(0.001)\n",
    "    # Initialize the critic cost\n",
    "    critic_cost = 0\n",
    "    # Initialize the iteration counter\n",
    "    k = 0\n",
    "    # Initialize the total reward\n",
    "    total_reward = 0\n",
    "    # Actor training\n",
    "    while critic_cost < critic_cost_bound && k < n_iters\n",
    "        # Sample a batch of transitions from the replay buffer\n",
    "        batch = sample(replay_buffer, n_samples)\n",
    "        # Calculate the target Q value\n",
    "        q_target = batch[1:end,4] + γ * q̂(batch[1:end,3])\n",
    "        # Update the critic network\n",
    "        gs = gradient(() -> crossentropy(q(batch[1:end,1:2]), q_target), params(q))\n",
    "        update!(opt_q, params(q), gs)\n",
    "        # Update the actor network\n",
    "        gs = gradient(() -> -q(p(batch[1:end,1])), params(p))\n",
    "        update!(opt_p, params(p), gs)\n",
    "        # Update the target networks\n",
    "        p̂ = deepcopy(p)\n",
    "        q̂ = deepcopy(q)\n",
    "        # Update the critic cost\n",
    "        critic_cost = crossentropy(q(batch[1:end,1:2]), q_target)\n",
    "        # Update the iteration counter\n",
    "        k += 1\n",
    "    end\n",
    "    # Play the game\n",
    "    log = play(p; x_init, n_steps, ap, sp, ϵ=0.0)\n",
    "    # Calculate the total reward\n",
    "    total_reward = sum([log[i].sc for i in 1:n_steps])\n",
    "    return total_reward\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = [0.0, 0.0, 0.0, 0.0]\n",
    "q_layers = Chain(Dense(2, 4, relu), Dense(4, 2, relu), Dense(2, 2), softmax)\n",
    "p_layers = Chain(Dense(2, 4, relu), Dense(4, 2, relu), Dense(2, 2), softmax)\n",
    "n_iters = 40\n",
    "n_steps = 1000\n",
    "n_critic_pre_steps = 100\n",
    "n_samples = 80\n",
    "replay_buffer_size = 100_000\n",
    "critic_cost_bound = 1000\n",
    "γ = 0.05\n",
    "ap = 1\n",
    "sp = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
